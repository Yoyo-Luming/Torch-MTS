METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- MTGNN ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        100
    ],
    "clip_grad": 5,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": true,
    "cl_step_size": 2500,
    "pass_device": true,
    "model_args": {
        "num_nodes": 207,
        "in_dim": 2,
        "seq_length": 12,
        "out_dim": 12,
        "device": "cuda:0",
        "gcn_true": true,
        "buildA_true": true,
        "gcn_depth": 2,
        "predefined_A": null,
        "static_feat": null,
        "dropout": 0.3,
        "subgraph_size": 20,
        "node_dim": 40,
        "dilation_exponential": 1,
        "conv_channels": 32,
        "residual_channels": 32,
        "skip_channels": 64,
        "end_channels": 128,
        "layers": 3,
        "propalpha": 0.05,
        "tanhalpha": 3,
        "layer_norm_affline": true,
        "node_emb_file": "../data/METRLA/spatial_embeddings.npz",
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 7,
        "node_embedding_dim": 64,
        "learner_hidden_dim": 128,
        "z_dim": 64,
        "add_meta_adj": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MTGNN                                    [64, 12, 207, 1]          29,664
├─Sequential: 1-1                        [64, 207, 64]             --
│    └─Linear: 2-1                       [64, 207, 32]             416
│    └─Tanh: 2-2                         [64, 207, 32]             --
│    └─Linear: 2-3                       [64, 207, 32]             1,056
│    └─Tanh: 2-4                         [64, 207, 32]             --
│    └─Linear: 2-5                       [64, 207, 64]             2,112
├─Sequential: 1-2                        [64, 207, 64]             --
│    └─Linear: 2-6                       [64, 207, 32]             416
│    └─Tanh: 2-7                         [64, 207, 32]             --
│    └─Linear: 2-8                       [64, 207, 32]             1,056
│    └─Tanh: 2-9                         [64, 207, 32]             --
│    └─Linear: 2-10                      [64, 207, 64]             2,112
├─Sequential: 1-3                        [64, 207, 40]             --
│    └─Linear: 2-11                      [64, 207, 128]            20,480
│    └─ReLU: 2-12                        [64, 207, 128]            --
│    └─Linear: 2-13                      [64, 207, 40]             5,160
├─Sequential: 1-4                        [64, 207, 40]             --
│    └─Linear: 2-14                      [64, 207, 128]            20,480
│    └─ReLU: 2-15                        [64, 207, 128]            --
│    └─Linear: 2-16                      [64, 207, 40]             5,160
├─graph_constructor: 1-5                 [64, 207, 207]            1,640
│    └─Embedding: 2-17                   [207, 40]                 8,280
│    └─Embedding: 2-18                   [207, 40]                 8,280
│    └─Linear: 2-19                      [64, 207, 40]             1,640
│    └─Linear: 2-20                      [64, 207, 40]             (recursive)
├─Conv2d: 1-6                            [64, 32, 207, 19]         96
├─Conv2d: 1-7                            [64, 64, 207, 1]          2,496
├─ModuleList: 1-20                       --                        (recursive)
│    └─dilated_inception: 2-21           [64, 32, 207, 13]         --
│    │    └─ModuleList: 3-1              --                        4,640
├─ModuleList: 1-21                       --                        (recursive)
│    └─dilated_inception: 2-22           [64, 32, 207, 13]         --
│    │    └─ModuleList: 3-2              --                        4,640
├─ModuleList: 1-22                       --                        (recursive)
│    └─Conv2d: 2-23                      [64, 64, 207, 1]          26,688
├─ModuleList: 1-23                       --                        (recursive)
│    └─mixprop: 2-24                     [64, 32, 207, 13]         --
│    │    └─nconv: 3-3                   [64, 32, 207, 13]         --
│    │    └─nconv: 3-4                   [64, 32, 207, 13]         --
│    │    └─linear: 3-5                  [64, 32, 207, 13]         3,104
├─ModuleList: 1-24                       --                        (recursive)
│    └─mixprop: 2-25                     [64, 32, 207, 13]         --
│    │    └─nconv: 3-6                   [64, 32, 207, 13]         --
│    │    └─nconv: 3-7                   [64, 32, 207, 13]         --
│    │    └─linear: 3-8                  [64, 32, 207, 13]         3,104
├─ModuleList: 1-25                       --                        (recursive)
│    └─LayerNorm: 2-26                   [64, 32, 207, 13]         172,224
├─ModuleList: 1-20                       --                        (recursive)
│    └─dilated_inception: 2-27           [64, 32, 207, 7]          --
│    │    └─ModuleList: 3-9              --                        4,640
├─ModuleList: 1-21                       --                        (recursive)
│    └─dilated_inception: 2-28           [64, 32, 207, 7]          --
│    │    └─ModuleList: 3-10             --                        4,640
├─ModuleList: 1-22                       --                        (recursive)
│    └─Conv2d: 2-29                      [64, 64, 207, 1]          14,400
├─ModuleList: 1-23                       --                        (recursive)
│    └─mixprop: 2-30                     [64, 32, 207, 7]          --
│    │    └─nconv: 3-11                  [64, 32, 207, 7]          --
│    │    └─nconv: 3-12                  [64, 32, 207, 7]          --
│    │    └─linear: 3-13                 [64, 32, 207, 7]          3,104
├─ModuleList: 1-24                       --                        (recursive)
│    └─mixprop: 2-31                     [64, 32, 207, 7]          --
│    │    └─nconv: 3-14                  [64, 32, 207, 7]          --
│    │    └─nconv: 3-15                  [64, 32, 207, 7]          --
│    │    └─linear: 3-16                 [64, 32, 207, 7]          3,104
├─ModuleList: 1-25                       --                        (recursive)
│    └─LayerNorm: 2-32                   [64, 32, 207, 7]          92,736
├─ModuleList: 1-20                       --                        (recursive)
│    └─dilated_inception: 2-33           [64, 32, 207, 1]          --
│    │    └─ModuleList: 3-17             --                        4,640
├─ModuleList: 1-21                       --                        (recursive)
│    └─dilated_inception: 2-34           [64, 32, 207, 1]          --
│    │    └─ModuleList: 3-18             --                        4,640
├─ModuleList: 1-22                       --                        (recursive)
│    └─Conv2d: 2-35                      [64, 64, 207, 1]          2,112
├─ModuleList: 1-23                       --                        (recursive)
│    └─mixprop: 2-36                     [64, 32, 207, 1]          --
│    │    └─nconv: 3-19                  [64, 32, 207, 1]          --
│    │    └─nconv: 3-20                  [64, 32, 207, 1]          --
│    │    └─linear: 3-21                 [64, 32, 207, 1]          3,104
├─ModuleList: 1-24                       --                        (recursive)
│    └─mixprop: 2-37                     [64, 32, 207, 1]          --
│    │    └─nconv: 3-22                  [64, 32, 207, 1]          --
│    │    └─nconv: 3-23                  [64, 32, 207, 1]          --
│    │    └─linear: 3-24                 [64, 32, 207, 1]          3,104
├─ModuleList: 1-25                       --                        (recursive)
│    └─LayerNorm: 2-38                   [64, 32, 207, 1]          13,248
├─Conv2d: 1-26                           [64, 64, 207, 1]          2,112
├─Conv2d: 1-27                           [64, 128, 207, 1]         8,320
├─Conv2d: 1-28                           [64, 12, 207, 1]          1,548
==========================================================================================
Total params: 490,396
Trainable params: 490,396
Non-trainable params: 0
Total mult-adds (G): 5.70
==========================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 587.28
Params size (MB): 1.84
Estimated Total Size (MB): 591.03
==========================================================================================

Loss: MaskedMAELoss

CL target length = 1
2023-04-25 10:09:00.982765 Epoch 1  	Train Loss = 2.60455 Val Loss = 10.60196
2023-04-25 10:09:45.163627 Epoch 2  	Train Loss = 2.38267 Val Loss = 10.59846
2023-04-25 10:10:29.322629 Epoch 3  	Train Loss = 2.34180 Val Loss = 10.59680
2023-04-25 10:11:13.574397 Epoch 4  	Train Loss = 2.31447 Val Loss = 10.59451
2023-04-25 10:11:57.767954 Epoch 5  	Train Loss = 2.29753 Val Loss = 10.59388
2023-04-25 10:12:41.995741 Epoch 6  	Train Loss = 2.28834 Val Loss = 10.59291
CL target length = 2
2023-04-25 10:13:26.215989 Epoch 7  	Train Loss = 2.41013 Val Loss = 9.85013
2023-04-25 10:14:10.335786 Epoch 8  	Train Loss = 2.43213 Val Loss = 9.85046
2023-04-25 10:14:54.418026 Epoch 9  	Train Loss = 2.42137 Val Loss = 9.84807
2023-04-25 10:15:38.585886 Epoch 10  	Train Loss = 2.40981 Val Loss = 9.84736
2023-04-25 10:16:22.648218 Epoch 11  	Train Loss = 2.40268 Val Loss = 9.84772
2023-04-25 10:17:06.685402 Epoch 12  	Train Loss = 2.39704 Val Loss = 9.84697
2023-04-25 10:17:50.731972 Epoch 13  	Train Loss = 2.39085 Val Loss = 9.84709
CL target length = 3
2023-04-25 10:18:34.906419 Epoch 14  	Train Loss = 2.52749 Val Loss = 9.11922
2023-04-25 10:19:18.981544 Epoch 15  	Train Loss = 2.50455 Val Loss = 9.11943
2023-04-25 10:20:03.065300 Epoch 16  	Train Loss = 2.49646 Val Loss = 9.11241
2023-04-25 10:20:47.283549 Epoch 17  	Train Loss = 2.49190 Val Loss = 9.11537
2023-04-25 10:21:31.523698 Epoch 18  	Train Loss = 2.48476 Val Loss = 9.10920
2023-04-25 10:22:15.666689 Epoch 19  	Train Loss = 2.48039 Val Loss = 9.11102
CL target length = 4
2023-04-25 10:22:59.989252 Epoch 20  	Train Loss = 2.47943 Val Loss = 9.06424
2023-04-25 10:23:44.040479 Epoch 21  	Train Loss = 2.60443 Val Loss = 8.38756
2023-04-25 10:24:28.154560 Epoch 22  	Train Loss = 2.55866 Val Loss = 8.38597
2023-04-25 10:25:12.285887 Epoch 23  	Train Loss = 2.54774 Val Loss = 8.38657
2023-04-25 10:25:56.312098 Epoch 24  	Train Loss = 2.54459 Val Loss = 8.37968
2023-04-25 10:26:40.329474 Epoch 25  	Train Loss = 2.53423 Val Loss = 8.38167
2023-04-25 10:27:24.415709 Epoch 26  	Train Loss = 2.52888 Val Loss = 8.38437
CL target length = 5
2023-04-25 10:28:08.605914 Epoch 27  	Train Loss = 2.59287 Val Loss = 7.66279
2023-04-25 10:28:52.786813 Epoch 28  	Train Loss = 2.60014 Val Loss = 7.65812
2023-04-25 10:29:36.958944 Epoch 29  	Train Loss = 2.59428 Val Loss = 7.65823
2023-04-25 10:30:21.117400 Epoch 30  	Train Loss = 2.58732 Val Loss = 7.65445
2023-04-25 10:31:05.321824 Epoch 31  	Train Loss = 2.57856 Val Loss = 7.65801
2023-04-25 10:31:49.345825 Epoch 32  	Train Loss = 2.57455 Val Loss = 7.65730
2023-04-25 10:32:33.517565 Epoch 33  	Train Loss = 2.56685 Val Loss = 7.65177
CL target length = 6
2023-04-25 10:33:17.524940 Epoch 34  	Train Loss = 2.64350 Val Loss = 6.93904
2023-04-25 10:34:01.451812 Epoch 35  	Train Loss = 2.62778 Val Loss = 6.94279
2023-04-25 10:34:45.462172 Epoch 36  	Train Loss = 2.61773 Val Loss = 6.93803
2023-04-25 10:35:29.501446 Epoch 37  	Train Loss = 2.61075 Val Loss = 6.93617
2023-04-25 10:36:13.489714 Epoch 38  	Train Loss = 2.60728 Val Loss = 6.94521
2023-04-25 10:36:57.522896 Epoch 39  	Train Loss = 2.60318 Val Loss = 6.93580
CL target length = 7
2023-04-25 10:37:41.736980 Epoch 40  	Train Loss = 2.59991 Val Loss = 6.90132
2023-04-25 10:38:25.946904 Epoch 41  	Train Loss = 2.68164 Val Loss = 6.23892
2023-04-25 10:39:10.058599 Epoch 42  	Train Loss = 2.65067 Val Loss = 6.22987
2023-04-25 10:39:54.115097 Epoch 43  	Train Loss = 2.64626 Val Loss = 6.22909
2023-04-25 10:40:38.108347 Epoch 44  	Train Loss = 2.63949 Val Loss = 6.24017
2023-04-25 10:41:22.052075 Epoch 45  	Train Loss = 2.63672 Val Loss = 6.22585
2023-04-25 10:42:06.294072 Epoch 46  	Train Loss = 2.63493 Val Loss = 6.23001
CL target length = 8
2023-04-25 10:42:50.535363 Epoch 47  	Train Loss = 2.67493 Val Loss = 5.52578
2023-04-25 10:43:34.769185 Epoch 48  	Train Loss = 2.68056 Val Loss = 5.52258
2023-04-25 10:44:19.104560 Epoch 49  	Train Loss = 2.67369 Val Loss = 5.54048
2023-04-25 10:45:03.552036 Epoch 50  	Train Loss = 2.67072 Val Loss = 5.52409
2023-04-25 10:45:47.970439 Epoch 51  	Train Loss = 2.66677 Val Loss = 5.54478
2023-04-25 10:46:32.214023 Epoch 52  	Train Loss = 2.66260 Val Loss = 5.52572
2023-04-25 10:47:16.521895 Epoch 53  	Train Loss = 2.65951 Val Loss = 5.53144
CL target length = 9
2023-04-25 10:48:00.732091 Epoch 54  	Train Loss = 2.71498 Val Loss = 4.84038
2023-04-25 10:48:45.084746 Epoch 55  	Train Loss = 2.70263 Val Loss = 4.86328
2023-04-25 10:49:29.504920 Epoch 56  	Train Loss = 2.69623 Val Loss = 4.84077
2023-04-25 10:50:14.031248 Epoch 57  	Train Loss = 2.69246 Val Loss = 4.83597
2023-04-25 10:50:58.552566 Epoch 58  	Train Loss = 2.69076 Val Loss = 4.83651
2023-04-25 10:51:42.745835 Epoch 59  	Train Loss = 2.68697 Val Loss = 4.82682
CL target length = 10
2023-04-25 10:52:27.005720 Epoch 60  	Train Loss = 2.68879 Val Loss = 4.79598
2023-04-25 10:53:10.979489 Epoch 61  	Train Loss = 2.74444 Val Loss = 4.16196
2023-04-25 10:53:54.905213 Epoch 62  	Train Loss = 2.72073 Val Loss = 4.14441
2023-04-25 10:54:38.875694 Epoch 63  	Train Loss = 2.71311 Val Loss = 4.15794
2023-04-25 10:55:22.807818 Epoch 64  	Train Loss = 2.71509 Val Loss = 4.14542
2023-04-25 10:56:06.825823 Epoch 65  	Train Loss = 2.70967 Val Loss = 4.14834
2023-04-25 10:56:50.840235 Epoch 66  	Train Loss = 2.70686 Val Loss = 4.15402
CL target length = 11
2023-04-25 10:57:34.855414 Epoch 67  	Train Loss = 2.74047 Val Loss = 3.46768
2023-04-25 10:58:18.860056 Epoch 68  	Train Loss = 2.74372 Val Loss = 3.46855
2023-04-25 10:59:02.847744 Epoch 69  	Train Loss = 2.73665 Val Loss = 3.47536
2023-04-25 10:59:46.856476 Epoch 70  	Train Loss = 2.73163 Val Loss = 3.46143
2023-04-25 11:00:30.940100 Epoch 71  	Train Loss = 2.73048 Val Loss = 3.47372
2023-04-25 11:01:15.047045 Epoch 72  	Train Loss = 2.72885 Val Loss = 3.46936
2023-04-25 11:01:59.055521 Epoch 73  	Train Loss = 2.72529 Val Loss = 3.44393
CL target length = 12
2023-04-25 11:02:43.138004 Epoch 74  	Train Loss = 2.76318 Val Loss = 2.78213
2023-04-25 11:03:27.105824 Epoch 75  	Train Loss = 2.75600 Val Loss = 2.77821
2023-04-25 11:04:11.049081 Epoch 76  	Train Loss = 2.75137 Val Loss = 2.79334
2023-04-25 11:04:55.135543 Epoch 77  	Train Loss = 2.74797 Val Loss = 2.79298
2023-04-25 11:05:39.296378 Epoch 78  	Train Loss = 2.74638 Val Loss = 2.78638
2023-04-25 11:06:23.272797 Epoch 79  	Train Loss = 2.74429 Val Loss = 2.77204
2023-04-25 11:07:07.111126 Epoch 80  	Train Loss = 2.74367 Val Loss = 2.79936
2023-04-25 11:07:51.052916 Epoch 81  	Train Loss = 2.73946 Val Loss = 2.79177
2023-04-25 11:08:35.081503 Epoch 82  	Train Loss = 2.73561 Val Loss = 2.78203
2023-04-25 11:09:19.074153 Epoch 83  	Train Loss = 2.73516 Val Loss = 2.80179
2023-04-25 11:10:03.132531 Epoch 84  	Train Loss = 2.73374 Val Loss = 2.78663
2023-04-25 11:10:47.253945 Epoch 85  	Train Loss = 2.73297 Val Loss = 2.78620
2023-04-25 11:11:31.428122 Epoch 86  	Train Loss = 2.73034 Val Loss = 2.78590
2023-04-25 11:12:15.511318 Epoch 87  	Train Loss = 2.72670 Val Loss = 2.78806
2023-04-25 11:12:59.559745 Epoch 88  	Train Loss = 2.72747 Val Loss = 2.79802
2023-04-25 11:13:43.686265 Epoch 89  	Train Loss = 2.72527 Val Loss = 2.76976
2023-04-25 11:14:27.853699 Epoch 90  	Train Loss = 2.72396 Val Loss = 2.79315
2023-04-25 11:15:12.039922 Epoch 91  	Train Loss = 2.72024 Val Loss = 2.78497
2023-04-25 11:15:56.052933 Epoch 92  	Train Loss = 2.72073 Val Loss = 2.76751
2023-04-25 11:16:40.260788 Epoch 93  	Train Loss = 2.71959 Val Loss = 2.77851
2023-04-25 11:17:24.361918 Epoch 94  	Train Loss = 2.71667 Val Loss = 2.79758
2023-04-25 11:18:08.592354 Epoch 95  	Train Loss = 2.71614 Val Loss = 2.77146
2023-04-25 11:18:52.776952 Epoch 96  	Train Loss = 2.71570 Val Loss = 2.78106
2023-04-25 11:19:37.186592 Epoch 97  	Train Loss = 2.71240 Val Loss = 2.76639
2023-04-25 11:20:21.475248 Epoch 98  	Train Loss = 2.71270 Val Loss = 2.81104
2023-04-25 11:21:05.806898 Epoch 99  	Train Loss = 2.70772 Val Loss = 2.79601
2023-04-25 11:21:50.063705 Epoch 100  	Train Loss = 2.70848 Val Loss = 2.77723
2023-04-25 11:22:34.230177 Epoch 101  	Train Loss = 2.66474 Val Loss = 2.77297
2023-04-25 11:23:18.597718 Epoch 102  	Train Loss = 2.65411 Val Loss = 2.77084
2023-04-25 11:24:02.979069 Epoch 103  	Train Loss = 2.65027 Val Loss = 2.77157
2023-04-25 11:24:47.255842 Epoch 104  	Train Loss = 2.64807 Val Loss = 2.76663
2023-04-25 11:25:31.452529 Epoch 105  	Train Loss = 2.64559 Val Loss = 2.77019
2023-04-25 11:26:15.891212 Epoch 106  	Train Loss = 2.64441 Val Loss = 2.76958
2023-04-25 11:27:00.119605 Epoch 107  	Train Loss = 2.64329 Val Loss = 2.77206
Early stopping at epoch: 107
Best at epoch 97:
Train Loss = 2.71240
Train RMSE = 5.13452, MAE = 2.59250, MAPE = 6.74796
Val Loss = 2.76639
Val RMSE = 5.87930, MAE = 2.80782, MAPE = 7.82331
--------- Test ---------
All Steps RMSE = 6.21980, MAE = 3.03360, MAPE = 8.33212
Step 1 RMSE = 4.03310, MAE = 2.28965, MAPE = 5.62059
Step 2 RMSE = 4.80364, MAE = 2.55461, MAPE = 6.51728
Step 3 RMSE = 5.31457, MAE = 2.72976, MAPE = 7.14168
Step 4 RMSE = 5.71052, MAE = 2.86656, MAPE = 7.66549
Step 5 RMSE = 6.02912, MAE = 2.97832, MAPE = 8.09838
Step 6 RMSE = 6.28277, MAE = 3.07349, MAPE = 8.46156
Step 7 RMSE = 6.51358, MAE = 3.15797, MAPE = 8.78409
Step 8 RMSE = 6.71282, MAE = 3.23173, MAPE = 9.07338
Step 9 RMSE = 6.88225, MAE = 3.29671, MAPE = 9.33295
Step 10 RMSE = 7.02816, MAE = 3.35388, MAPE = 9.56441
Step 11 RMSE = 7.15099, MAE = 3.40783, MAPE = 9.76597
Step 12 RMSE = 7.26828, MAE = 3.46283, MAPE = 9.96004
Inference time: 4.85 s
