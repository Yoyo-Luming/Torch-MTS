METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        40
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 64,
    "max_epochs": 200,
    "early_stop": 20,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 12,
        "tod_embedding_dim": 12,
        "dow_embedding_dim": 12,
        "adaptive_embedding_dim": 60,
        "feed_forward_dim": 128,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAttention                              --                        --
├─ModuleList: 1-1                        --                        --
│    └─ModuleList: 2-1                   --                        --
│    └─ModuleList: 2-2                   --                        --
│    └─ModuleList: 2-3                   --                        --
├─ModuleList: 1-2                        --                        --
│    └─ModuleList: 2-4                   --                        --
│    └─ModuleList: 2-5                   --                        --
│    └─ModuleList: 2-6                   --                        --
├─Linear: 1-3                            [64, 12, 207, 12]         24
├─Embedding: 1-4                         [64, 12, 207, 12]         3,456
├─Embedding: 1-5                         [64, 12, 207, 12]         84
├─ModuleList: 1-1                        --                        --
│    └─ModuleList: 2-1                   --                        --
│    │    └─LayerNorm: 3-1               [64, 12, 207, 96]         192
│    │    └─Sequential: 3-2              [64, 12, 207, 96]         24,800
│    │    └─LayerNorm: 3-3               [64, 12, 207, 96]         192
│    └─ModuleList: 2-2                   --                        --
│    │    └─LayerNorm: 3-4               [64, 12, 207, 96]         192
│    │    └─Sequential: 3-5              [64, 12, 207, 96]         24,800
│    │    └─LayerNorm: 3-6               [64, 12, 207, 96]         192
│    └─ModuleList: 2-3                   --                        --
│    │    └─LayerNorm: 3-7               [64, 12, 207, 96]         192
│    │    └─Sequential: 3-8              [64, 12, 207, 96]         24,800
│    │    └─LayerNorm: 3-9               [64, 12, 207, 96]         192
├─ModuleList: 1-2                        --                        --
│    └─ModuleList: 2-4                   --                        --
│    │    └─LayerNorm: 3-10              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-11             [64, 12, 207, 96]         24,800
│    │    └─LayerNorm: 3-12              [64, 12, 207, 96]         192
│    └─ModuleList: 2-5                   --                        --
│    │    └─LayerNorm: 3-13              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-14             [64, 12, 207, 96]         24,800
│    │    └─LayerNorm: 3-15              [64, 12, 207, 96]         192
│    └─ModuleList: 2-6                   --                        --
│    │    └─LayerNorm: 3-16              [64, 12, 207, 96]         192
│    │    └─Sequential: 3-17             [64, 12, 207, 96]         24,800
│    │    └─LayerNorm: 3-18              [64, 12, 207, 96]         192
├─Linear: 1-6                            [64, 96, 207, 12]         156
├─Linear: 1-7                            [64, 12, 207, 1]          97
==========================================================================================
Total params: 154,921
Trainable params: 154,921
Non-trainable params: 0
Total mult-adds (M): 9.91
==========================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 3343.58
Params size (MB): 0.62
Estimated Total Size (MB): 3346.11
==========================================================================================

Loss: MaskedMAELoss

2023-04-14 11:15:15.604456 Epoch 1  	Train Loss = 4.99084 Val Loss = 3.63935
2023-04-14 11:15:51.259592 Epoch 2  	Train Loss = 3.68608 Val Loss = 3.36856
2023-04-14 11:16:27.143325 Epoch 3  	Train Loss = 3.46499 Val Loss = 3.31635
2023-04-14 11:17:02.766803 Epoch 4  	Train Loss = 3.37130 Val Loss = 3.20629
2023-04-14 11:17:38.626594 Epoch 5  	Train Loss = 3.28149 Val Loss = 3.11649
2023-04-14 11:18:14.490187 Epoch 6  	Train Loss = 3.20504 Val Loss = 3.11791
2023-04-14 11:18:50.557022 Epoch 7  	Train Loss = 3.15407 Val Loss = 3.06342
2023-04-14 11:19:26.722972 Epoch 8  	Train Loss = 3.10993 Val Loss = 3.04119
2023-04-14 11:20:02.899805 Epoch 9  	Train Loss = 3.07636 Val Loss = 3.02548
2023-04-14 11:20:39.016053 Epoch 10  	Train Loss = 3.04797 Val Loss = 3.04549
2023-04-14 11:21:15.048348 Epoch 11  	Train Loss = 3.03270 Val Loss = 3.01710
2023-04-14 11:21:51.046993 Epoch 12  	Train Loss = 3.01852 Val Loss = 2.99666
2023-04-14 11:22:26.994532 Epoch 13  	Train Loss = 3.00481 Val Loss = 2.98278
2023-04-14 11:23:03.037083 Epoch 14  	Train Loss = 2.99483 Val Loss = 2.97590
2023-04-14 11:23:39.098077 Epoch 15  	Train Loss = 2.98424 Val Loss = 2.98268
2023-04-14 11:24:15.134233 Epoch 16  	Train Loss = 2.97563 Val Loss = 2.99215
2023-04-14 11:24:51.176610 Epoch 17  	Train Loss = 2.96960 Val Loss = 2.99095
2023-04-14 11:25:27.228485 Epoch 18  	Train Loss = 2.96421 Val Loss = 2.99234
2023-04-14 11:26:03.287016 Epoch 19  	Train Loss = 2.95706 Val Loss = 2.97190
2023-04-14 11:26:39.267327 Epoch 20  	Train Loss = 2.95042 Val Loss = 2.96976
2023-04-14 11:27:15.268377 Epoch 21  	Train Loss = 2.94402 Val Loss = 2.96412
2023-04-14 11:27:51.438388 Epoch 22  	Train Loss = 2.94201 Val Loss = 2.99824
2023-04-14 11:28:27.558003 Epoch 23  	Train Loss = 2.93520 Val Loss = 2.96425
2023-04-14 11:29:03.592656 Epoch 24  	Train Loss = 2.93097 Val Loss = 2.95603
2023-04-14 11:29:39.577688 Epoch 25  	Train Loss = 2.92985 Val Loss = 2.99484
2023-04-14 11:30:15.590608 Epoch 26  	Train Loss = 2.92052 Val Loss = 2.96963
2023-04-14 11:30:51.972408 Epoch 27  	Train Loss = 2.91665 Val Loss = 2.98221
2023-04-14 11:31:28.063266 Epoch 28  	Train Loss = 2.91708 Val Loss = 2.97945
2023-04-14 11:32:04.113068 Epoch 29  	Train Loss = 2.90985 Val Loss = 2.99090
2023-04-14 11:32:40.158480 Epoch 30  	Train Loss = 2.91025 Val Loss = 2.97736
2023-04-14 11:33:16.303287 Epoch 31  	Train Loss = 2.90289 Val Loss = 2.98155
2023-04-14 11:33:52.339121 Epoch 32  	Train Loss = 2.90171 Val Loss = 2.97193
2023-04-14 11:34:28.624677 Epoch 33  	Train Loss = 2.89813 Val Loss = 2.98669
2023-04-14 11:35:04.628482 Epoch 34  	Train Loss = 2.89613 Val Loss = 2.98637
2023-04-14 11:35:40.702317 Epoch 35  	Train Loss = 2.89161 Val Loss = 2.97797
2023-04-14 11:36:16.861500 Epoch 36  	Train Loss = 2.89168 Val Loss = 2.98259
2023-04-14 11:36:52.980072 Epoch 37  	Train Loss = 2.88659 Val Loss = 2.98255
2023-04-14 11:37:28.944643 Epoch 38  	Train Loss = 2.88535 Val Loss = 2.97717
2023-04-14 11:38:04.917790 Epoch 39  	Train Loss = 2.88224 Val Loss = 2.97659
2023-04-14 11:38:40.864338 Epoch 40  	Train Loss = 2.88381 Val Loss = 2.98132
2023-04-14 11:39:16.904803 Epoch 41  	Train Loss = 2.83979 Val Loss = 2.96824
2023-04-14 11:39:52.864319 Epoch 42  	Train Loss = 2.82969 Val Loss = 2.97137
2023-04-14 11:40:28.891499 Epoch 43  	Train Loss = 2.82709 Val Loss = 2.97502
2023-04-14 11:41:04.985105 Epoch 44  	Train Loss = 2.82407 Val Loss = 2.97248
Early stopping at epoch: 44
Best at epoch 24:
Train Loss = 2.93097
Train RMSE = 5.76839, MAE = 2.81819, MAPE = 7.66380
Val Loss = 2.95603
Val RMSE = 6.34818, MAE = 3.02266, MAPE = 8.72664
--------- Test ---------
All Steps RMSE = 6.59574, MAE = 3.17380, MAPE = 9.30969
Step 1 RMSE = 4.54696, MAE = 2.45754, MAPE = 6.33349
Step 2 RMSE = 5.18726, MAE = 2.67136, MAPE = 7.12257
Step 3 RMSE = 5.63879, MAE = 2.83823, MAPE = 7.77859
Step 4 RMSE = 5.99723, MAE = 2.98721, MAPE = 8.42342
Step 5 RMSE = 6.32317, MAE = 3.11725, MAPE = 9.02459
Step 6 RMSE = 6.64732, MAE = 3.22683, MAPE = 9.54491
Step 7 RMSE = 6.92891, MAE = 3.31463, MAPE = 9.95145
Step 8 RMSE = 7.13001, MAE = 3.38455, MAPE = 10.25738
Step 9 RMSE = 7.27032, MAE = 3.44299, MAPE = 10.50499
Step 10 RMSE = 7.40095, MAE = 3.49576, MAPE = 10.72302
Step 11 RMSE = 7.54200, MAE = 3.54676, MAPE = 10.92386
Step 12 RMSE = 7.69243, MAE = 3.60258, MAPE = 11.12828
Inference time: 3.15 s
